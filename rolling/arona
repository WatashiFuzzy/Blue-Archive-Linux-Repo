import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Select model
model_name = "meta-llama/Llama-2-7b-chat-hf"

# 4-bit quantization config (reduces RAM usage)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model with 4-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"  # Automatically chooses CPU or GPU
)

def chat():
    print("\nðŸ”¹ **Llama-2-7B Chatbot (4-bit)** ðŸ”¹")
    print("Type 'exit' to quit.\n")

    while True:
        user_input = input("Sensei: ")
        if user_input.lower() == "exit":
            print("Arona: Goodbye, Sensei!")
            break

        # Tokenize input
        inputs = tokenizer(user_input, return_tensors="pt").to(model.device)

        # Generate response
        output = model.generate(**inputs, max_length=150)
        response = tokenizer.decode(output[0], skip_special_tokens=True)

        print(f"Arona: {response}\n")

if __name__ == "__main__":
    chat()